# Data Engineering Nanodegree Projects
Projects developed under Udacity's [Data Engineering Nanodegree](https://www.udacity.com/course/data-engineer-nanodegree--nd027) program.

## Project 1: [Data Modeling with Postgres](https://github.com/sertozlu/Data-Engineering-Nanodegree-Projects/tree/master/Data-Modeling-with-Postgres)
Apply data modeling with Postgres and build an ETL pipeline using Python. Define fact and dimension tables for a star schema for a particular analytic focus to model user activity data for a music streaming app. Write an ETL pipeline that transfers data from files in two local directories into tables in Postgres using Python and SQL.

## Project 2: [Data Modeling with Apache Cassandra](https://github.com/sertozlu/Data-Engineering-Nanodegree-Projects/tree/master/Data-Modeling-with-Apache-Cassandra)
Apply data modeling with Apache Cassandra and complete an ETL pipeline using Python. Model data by creating tables in Apache Cassandra to run queries. Part of the ETL pipeline that transfers data from a set of CSV files within a directory to create a streamlined CSV file to model and insert data into Apache Cassandra tables is provided.

## Project 3: [Data Warehouse](https://github.com/sertozlu/Data-Engineering-Nanodegree-Projects/tree/master/Data-Warehouse)
Apply data warehousing models and architectures to build an ETL pipeline for a database hosted on AWS Redshift. Load data from S3 to staging tables on Redshift and execute SQL statements that create the analytics tables from these staging tables.

## Project 4: [Data Lake](https://github.com/sertozlu/Data-Engineering-Nanodegree-Projects/tree/master/Data-Lake)
Use Spark (AWS EMR-based) to build an ETL pipeline for a data lake hosted on S3. Load data from S3, process the data into analytics tables using Spark, and load them back into S3. Deploy this Spark process on a cluster using AWS.

## Project 5: [Data Pipelines](https://github.com/sertozlu/Data-Engineering-Nanodegree-Projects/tree/master/Data-Pipelines)
Use Apache Airflow to introduce more automation and monitoring to data warehouse ETL pipelines. Create a high grade data pipelines that are dynamic and built from reusable tasks, can be monitored, and allow easy backfills. Ensure data quality by running tests against datasets after the ETL steps have been executed to catch any discrepancies in the datasets.